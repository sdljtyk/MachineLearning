{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 决策树（分类）\n",
    "\n",
    "## 基本工作原理\n",
    "> 如下图所示流程图就是一个决策树。\n",
    "> + 正方形代表判断模块（decision block），椭圆形代表终止模块（terminating block），表示已经得出结论，可以终止运行。\n",
    "> + 从判断模块引出的左右箭头称作分支（branch），它可以到达另一个判断模块或者终止模块。\n",
    "\n",
    "> 下图构造了一个假想的邮件分类系统\n",
    "> + 它首先检测发送邮件域名地址。\n",
    "    * 如果地址为myEmployer.com，则将其放在分类“无聊时需要阅读的邮件”中。\n",
    "    * 如果邮件不是来自这个域名:\n",
    "        * 则检查邮件内容里是否包含单词曲棍球，\n",
    "            * 如果包含则将邮件归类到“需要及时处理的朋友邮件”;\n",
    "            * 如果不包含则将邮件归类到“无需阅读的垃圾邮件”。\n",
    "![title](./data/1.png)\n",
    "\n",
    "> 1. 第2章介绍的k-近邻算法可以完成很多分类任务，但是它最大的缺点就是无法给出数据的内在含义，决策树的主要优势就在于数据形式非常容易理解。\n",
    "> 2. 决策树很多任务都是为了数据中所蕴含的知识信息，因此决策树可以使用不熟悉的数据集合，并从中提取出一系列规则，机器学习算法最终将使用这些机器从数据集中创造的规则。\n",
    "\n",
    "## 特点\n",
    "> * 优点: 计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。\n",
    "> * 缺点：可能会产生过度匹配问题。\n",
    "> * 使用数据范围：数值型和标称型。\n",
    "\n",
    "## 一般流程\n",
    "> 1. 收集数据 ：可以使用任何方法\n",
    "> 2. 准备数据 : 树构造算法只适用于标称型数据，因此数值型数据必须离散化。\n",
    "> 3. 分析数据 : 可以使用任何方法，构造树完成之后，我们应该检查图形是否符合预期。\n",
    "> 4. 训练算法 ：构造树的数据结构。\n",
    "> 5. 测试算法 ：使用经验树计算错误率。\n",
    "> 6. 使用算法 ：此步骤可以适用于任何监督学习算法，而使用决策树可以更好地理解数据的内在含义。\n",
    "\n",
    "#### 注意：决策树需要解决第一个问题就是当前数据集上`哪个特征在划分数据分类时起决定性作用`，找到一个合适的特征评估指标，对每个特征进行评估，进而构建决策树。本章将采用ID3算法构建决策树，基于香农熵和信息增益进行特征的选择。  \n",
    "#### 在划分数据集之前之后信息发生的变化称为信息增益，知道如何计算信息增益，我们就可以计算每个特征值划分数据集获得的信息增益，获得信息增益最高的特征就是最好的选择。  \n",
    "> 熵定义为信息的期望值，在明晰这个概念之前，我们必须知道信息的定义。如果待分类的事务可能划分在多个分类之中，则样本集合D中第i类样本$ x_i $的信息定义为\n",
    "> $$ l(x_i)=-log_2p(x_i) $$\n",
    "> 其中$ p(x_i) $是选择该分类所占的比例。\n",
    "\n",
    "> 为了计算熵，我们需要计算所有类别所有可能值包含的信息期望值，通过下面的公式得到：\n",
    "> $$ H(D) = -\\sum^{n}_{i = 1}{p(x_i)log_2p(x_i)} $$\n",
    "> 熵的值越小，则样本集合的纯度越高。\n",
    "\n",
    "> 信息增益在决策树算法中是用来选择特征的指标，信息增益越大，则这个特征的选择性越好。计算特征a对样本集D进行划分所获得的信息增益为：\n",
    "> $$ Gain(D,a) = H(D) - \\sum_{v=1}^{V}{\\frac{D^v}{D}H(D^v)} $$\n",
    "> 其中：V表示依据特征a对样本集D划分后，获得的总共类别数量；$D^v $表示每一个新类别中样本数量。\n",
    "\n",
    "#### 事实上，信息增益准则对可取值数目较多的特征有所偏好，为了减少这种偏好可能带来的不利影响，C4.5决策树算法使用了“增益率”。CART分类树算法使用基尼系数来代替信息增益比，基尼系数代表了模型的不纯度，基尼系数越小，则不纯度越低，特征越好。这两种算法将在后面的学习中继续介绍。本章只介绍使用信息增益的ID3算法。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一个简单示例\n",
    "> 下图的数据包含5个海洋动物，特征包括：不浮出水面是否可以生存，以及是否有脚蹼。我们可以将这些动物分成两类：鱼类和非鱼类。现在我们想要决定依据第一个特征还是第二个特征划分数据。\n",
    "\n",
    "![title](./data/2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引入相关包\n",
    "from math import log\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成数据\n",
    "def createDataSet():\n",
    "    dataSet = [[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']]\n",
    "    labels = ['no surfacing', 'flippers']\n",
    "    return dataSet, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算香农熵\n",
    "def calcShannonEnt(dataSet):\n",
    "    numEntries = len(dataSet)\n",
    "    labelCounts = {}\n",
    "\n",
    "    for featVec in dataSet:\n",
    "        currentlabel = featVec[-1]\n",
    "        if currentlabel not in labelCounts.keys():\n",
    "            labelCounts[currentlabel] = 0\n",
    "            labelCounts[currentlabel] += 1\n",
    "    shannonEnt = 0.0\n",
    "    for key in labelCounts:\n",
    "        prob = float(labelCounts[key]) / numEntries\n",
    "        shannonEnt -= prob * log(prob, 2)\n",
    "\n",
    "    return shannonEnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分数据集\n",
    "\"\"\"\n",
    "dataSet:待划分的数据集\n",
    "axis:划分数据集的特征\n",
    "value:特征的返回值\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def splitDataSet(dataSet, axis, value):\n",
    "    retDataSet = []\n",
    "    for feaVec in dataSet:\n",
    "        if feaVec[axis] == value:\n",
    "            reducedFeatVec = feaVec[:axis]\n",
    "            reducedFeatVec.extend(feaVec[axis + 1:])\n",
    "            retDataSet.append(reducedFeatVec)\n",
    "    return retDataSet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 选择最好的数据集划分方式\n",
    "def chooseBestFeatureToSplit(dataSet):\n",
    "    numFeatures = len(dataSet[0]) - 1\n",
    "    baseEntropy = calcShannonEnt(dataSet)\n",
    "    bestInfoGain = 0.0\n",
    "    bestFeature = -1\n",
    "    for i in range(numFeatures):\n",
    "        featList = [example[i] for example in dataSet]\n",
    "        uniqueVals = set(featList)\n",
    "        newEntropy = 0.0\n",
    "        for value in uniqueVals:\n",
    "            subDataSet = splitDataSet(dataSet, i, value)\n",
    "            prob = len(subDataSet) / float(len(dataSet))\n",
    "            newEntropy += prob * calcShannonEnt(subDataSet)\n",
    "        infoGain = baseEntropy - newEntropy\n",
    "        if infoGain > bestInfoGain:\n",
    "            bestInfoGain = infoGain\n",
    "            bestFeature = i\n",
    "    return bestFeature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
